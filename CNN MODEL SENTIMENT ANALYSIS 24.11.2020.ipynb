{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load clean dataset\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split())for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode a list of lines\n",
    "def encode_text(tokenizer,lines,length):\n",
    "    #integer encode\n",
    "    encoded=tokenizer.texts_to_sequences(lines)\n",
    "    #pad encoded sequences\n",
    "    padded=pad_sequences(encoded,maxlen=length,padding='post')\n",
    "    return padded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "def define_model(length,vocab_size):\n",
    "    #channel 1\n",
    "    inputs1=Input(shape=(length,))\n",
    "    embedding1=Embedding(vocab_size,100)(inputs1)\n",
    "    conv1=Conv1D(filters=32,kernel_size=4,activation='relu')(embedding1)\n",
    "    drop1=Dropout(0.5)(conv1)\n",
    "    pool1=MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1=Flatten()(pool1)\n",
    "    #channel2\n",
    "    inputs2=Input(shape=(length,))\n",
    "    embedding2=Embedding(vocab_size,100)(inputs2)\n",
    "    conv2=Conv1D(filters=32,kernel_size=6,activation='relu')(embedding2)\n",
    "    drop2=Dropout(0.5)(conv2)\n",
    "    pool2=MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2=Flatten()(pool2)\n",
    "    #channel3\n",
    "    inputs3=Input(shape=(length,))\n",
    "    embedding3=Embedding(vocab_size,100)(inputs3)\n",
    "    conv3=Conv1D(filters=32,kernel_size=6,activation='relu')(embedding3)\n",
    "    drop3=Dropout(0.5)(conv3)\n",
    "    pool3=MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3=Flatten()(pool3)\n",
    "    \n",
    "    #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretation\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    outputs=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[inputs1,inputs2,inputs3],outputs=outputs)\n",
    "    \n",
    "    #compile\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #summarize\n",
    "    model.summary()\n",
    "    plot_model(model,show_shapes=True,to_file='model.png')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max doc length:1438\n",
      "voc size:73604\n"
     ]
    }
   ],
   "source": [
    "#load training dataset\n",
    "trainLines,trainLabels=load_dataset('train.pkl')\n",
    "#create tokenizer\n",
    "tokenizer=create_tokenizer(trainLines)\n",
    "#calculate max document length\n",
    "length=max_length(trainLines)\n",
    "print('Max doc length:%d' % length)\n",
    "\n",
    "#calculate vocabulary size\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print('voc size:%d' %vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 1438)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 1438)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 1438)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1438, 100)    7360400     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1438, 100)    7360400     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1438, 100)    7360400     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1435, 32)     12832       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 1433, 32)     19232       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1433, 32)     19232       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1435, 32)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1433, 32)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1433, 32)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 717, 32)      0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 716, 32)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 716, 32)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 22944)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 22912)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 22912)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 68768)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           687690      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            11          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 22,820,197\n",
      "Trainable params: 22,820,197\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/7\n",
      "1563/1563 [==============================] - 1442s 922ms/step - loss: 0.3621 - accuracy: 0.8257\n",
      "Epoch 2/7\n",
      "1563/1563 [==============================] - 1445s 924ms/step - loss: 0.1213 - accuracy: 0.9570\n",
      "Epoch 3/7\n",
      "1563/1563 [==============================] - 2890s 2s/step - loss: 0.0297 - accuracy: 0.9905\n",
      "Epoch 4/7\n",
      "1563/1563 [==============================] - 1149s 735ms/step - loss: 0.0099 - accuracy: 0.9969\n",
      "Epoch 5/7\n",
      "1563/1563 [==============================] - 1093s 699ms/step - loss: 0.0112 - accuracy: 0.9963\n",
      "Epoch 6/7\n",
      "1563/1563 [==============================] - 1065s 682ms/step - loss: 0.0105 - accuracy: 0.9963\n",
      "Epoch 7/7\n",
      "1563/1563 [==============================] - 1050s 672ms/step - loss: 0.0099 - accuracy: 0.9971\n"
     ]
    }
   ],
   "source": [
    "#encode data\n",
    "trainX=encode_text(tokenizer,trainLines,length)\n",
    "#define model\n",
    "model=define_model(length,vocab_size)\n",
    "#fit model\n",
    "model.fit([trainX,trainX,trainX],array(trainLabels),epochs=7,batch_size=16)\n",
    "#save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
